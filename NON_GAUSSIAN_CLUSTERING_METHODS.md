# Clustering Methods for Non-Gaussian Data

## Overview

Most classical clustering algorithms assume Gaussian (normal) distributions, but real-world data often violates this assumption. When your data has:
- Heavy tails (extreme values)
- Skewed distributions
- Multimodal clusters
- Non-elliptical shapes
- Arbitrary cluster geometries

...you need clustering methods designed for **non-Gaussian data**.

---

## 1. Density-Based Methods

### **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**

**Key Idea:** Clusters are dense regions separated by low-density regions.

**How it works:**
- Define two parameters: `eps` (neighborhood radius) and `min_samples`
- **Core points**: Points with ‚â• `min_samples` neighbors within `eps`
- **Border points**: Non-core points within `eps` of a core point
- **Noise**: Points that are neither core nor border

**Advantages:**
- ‚úì Finds arbitrary-shaped clusters
- ‚úì Identifies outliers/noise (label -1)
- ‚úì No need to specify number of clusters
- ‚úì Works with non-Gaussian distributions

**Disadvantages:**
- ‚úó Struggles with varying densities (one `eps` for all clusters)
- ‚úó High-dimensional data (curse of dimensionality affects density)
- ‚úó Sensitive to parameter choice

**When to use:**
- Clusters have varying shapes (not spherical/elliptical)
- Need outlier detection
- Clusters have similar density

**Python:**
```python
from sklearn.cluster import DBSCAN

clusterer = DBSCAN(eps=0.5, min_samples=10)
labels = clusterer.fit_predict(X)
```

---

### **HDBSCAN (Hierarchical DBSCAN)**

**Key Idea:** Build a hierarchy of clusterings at different densities, then extract optimal clusters.

**How it works:**
1. Build minimum spanning tree based on mutual reachability distance
2. Create hierarchy of clusters at all density levels
3. Extract "persistent" clusters using stability measure

**Advantages:**
- ‚úì All DBSCAN advantages
- ‚úì **Handles varying densities** (major improvement)
- ‚úì Automatic cluster extraction
- ‚úì More robust to parameter choice

**Disadvantages:**
- ‚úó Slower than DBSCAN (hierarchical construction)
- ‚úó Still struggles in very high dimensions
- ‚úó May label many points as noise if clusters are not well-separated

**When to use:**
- Clusters have different densities
- Unsure about optimal `eps` parameter
- Need robust outlier detection

**Why it failed for VAE latent space:**
- Latent clusters have **uniform density** (no gradients to exploit)
- VAE already "clustered" data ‚Üí compact, similar-density regions
- 26% noise classification loses signal (those points still have lithology!)

**Python:**
```python
import hdbscan

clusterer = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=10)
labels = clusterer.fit_predict(X)
```

---

## 2. Distribution-Based Methods

### **Gaussian Mixture Models (GMM)**

**Key Idea:** Data generated from mixture of K Gaussian distributions with different parameters.

**How it works:**
- Assume each cluster is a Gaussian with mean Œº_k and covariance Œ£_k
- EM algorithm finds optimal parameters
- Soft assignments: P(cluster k | point x)

**Covariance Types:**
- **`full`**: Each cluster has its own general covariance (elliptical, any orientation) ‚Üê **Most flexible**
- **`tied`**: All clusters share same covariance (parallel ellipses)
- **`diag`**: Diagonal covariance (axes-aligned ellipses)
- **`spherical`**: Single variance parameter (spherical, like K-Means but probabilistic)

**Advantages:**
- ‚úì **Elliptical clusters** (not just spherical)
- ‚úì **Soft assignments** (probabilistic)
- ‚úì Different cluster sizes/variances
- ‚úì Model selection via BIC/AIC

**Disadvantages:**
- ‚úó **Still assumes Gaussian components** (limitation for heavy-tailed data)
- ‚úó Sensitive to initialization
- ‚úó Can overfit with too many components

**When to use:**
- Clusters are roughly elliptical
- Need probabilistic assignments
- Data is approximately Gaussian (even if clusters aren't)

**Why it worked better than K-Means for VAE:**
- VAE latent clusters are **elliptical** (different variances in different directions)
- `full` covariance captures elongated cluster shapes
- +13% improvement shows clusters aren't perfectly spherical

**Python:**
```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=12, covariance_type='full')
labels = gmm.fit_predict(X)
```

---

### **t-Distributed Mixture Models**

**Key Idea:** Like GMM, but use **Student's t-distribution** instead of Gaussian.

**Why Student's t?**
- **Heavy tails** ‚Üí robust to outliers
- **Degrees of freedom (ŒΩ)** controls tail heaviness
  - ŒΩ‚Üí‚àû: Approaches Gaussian
  - ŒΩ=1: Cauchy distribution (very heavy tails)
  - ŒΩ=3-5: Common choice for robust clustering

**Advantages:**
- ‚úì **Robust to outliers** (heavy tails absorb extreme points)
- ‚úì Elliptical clusters (like GMM)
- ‚úì Better for non-Gaussian data with outliers

**Disadvantages:**
- ‚úó More parameters to estimate (ŒΩ for each component)
- ‚úó Slower than GMM
- ‚úó Harder to implement (not in sklearn)

**When to use:**
- Data has outliers/heavy tails
- GMM overfits to extreme points
- Robust clustering needed

**Python (requires external package):**
```python
# Option 1: pomegranate
from pomegranate import GeneralMixtureModel, MultivariateGaussianDistribution

# Option 2: scikit-learn-extra
from sklearn_extra.cluster import KMedoids  # Not t-mixture, but robust alternative
```

---

### **Skew-Normal Mixture Models**

**Key Idea:** Allow **asymmetric (skewed) distributions** in each cluster.

**Why skew-normal?**
- Gaussian assumes symmetry (mean = median = mode)
- Real data often has skewness (tail on one side)
- Skew-normal has **skewness parameter Œ±**:
  - Œ±=0: Standard Gaussian
  - Œ±>0: Right-skewed
  - Œ±<0: Left-skewed

**Advantages:**
- ‚úì Handles skewed clusters
- ‚úì More flexible than GMM
- ‚úì Reduces misclassification when clusters overlap asymmetrically

**Disadvantages:**
- ‚úó More complex (additional skewness parameters)
- ‚úó Not standard in sklearn
- ‚úó Harder to interpret

**When to use:**
- Clusters have visible skewness
- GMM forces symmetric clusters poorly
- Asymmetric overlaps between clusters

---

## 3. Non-Parametric Methods

### **Mean Shift**

**Key Idea:** Find modes (peaks) in the density function by iteratively shifting points toward higher density.

**How it works:**
1. For each point, compute mean of neighbors within bandwidth `h`
2. Shift point to that mean
3. Repeat until convergence
4. Points converging to same mode = same cluster

**Advantages:**
- ‚úì **No assumption about cluster shape**
- ‚úì Automatic number of clusters (finds all modes)
- ‚úì Works with arbitrary distributions
- ‚úì No random initialization

**Disadvantages:**
- ‚úó **Very slow** (O(n¬≤) for n points)
- ‚úó Sensitive to bandwidth `h`
- ‚úó Doesn't scale to high dimensions

**When to use:**
- Small-medium datasets
- Arbitrary cluster shapes
- Don't know number of clusters

**Python:**
```python
from sklearn.cluster import MeanShift

clusterer = MeanShift(bandwidth=0.5)
labels = clusterer.fit_predict(X)
```

---

### **Spectral Clustering**

**Key Idea:** Transform data into graph, then cluster in **spectral space** (eigenvectors of graph Laplacian).

**How it works:**
1. Build similarity graph (k-nearest neighbors or Œµ-neighborhoods)
2. Compute graph Laplacian matrix
3. Find k smallest eigenvectors
4. Run K-Means in eigenvector space

**Advantages:**
- ‚úì **Non-convex cluster shapes** (works with "moons", "circles", etc.)
- ‚úì Only needs similarity measure (not distances)
- ‚úì Effective for non-Gaussian manifold data

**Disadvantages:**
- ‚úó Need to specify k (number of clusters)
- ‚úó Sensitive to similarity graph construction
- ‚úó Computationally expensive (eigendecomposition)
- ‚úó Doesn't scale well (O(n¬≥))

**When to use:**
- Clusters are connected manifolds (not necessarily convex)
- Have good similarity/kernel function
- Moderate dataset size

**Python:**
```python
from sklearn.cluster import SpectralClustering

clusterer = SpectralClustering(n_clusters=12, affinity='nearest_neighbors')
labels = clusterer.fit_predict(X)
```

---

## 4. Subspace/Projected Clustering

### **Subspace Clustering**

**Key Idea:** Different clusters may exist in **different subspaces** of the feature space.

**Examples:**
- **CLIQUE**: Grid-based density in axis-parallel subspaces
- **SUBCLU**: DBSCAN in subspaces
- **PROCLUS**: Projected clustering with local dimensionality

**Why important:**
- High-dimensional data: clusters may only be meaningful in some dimensions
- Different features matter for different clusters

**Advantages:**
- ‚úì Handles high-dimensional data
- ‚úì Finds clusters in local subspaces
- ‚úì Avoids curse of dimensionality

**Disadvantages:**
- ‚úó Computationally expensive
- ‚úó Many parameters
- ‚úó Hard to interpret (which subspace?)

---

## 5. Model-Free Robust Methods

### **K-Medoids (PAM - Partitioning Around Medoids)**

**Key Idea:** Like K-Means, but use **medoids** (actual data points) as cluster centers instead of means.

**How it works:**
- Medoid = most central point in cluster (minimizes sum of dissimilarities)
- Robust to outliers (medoid is actual point, not affected by extremes)
- Can use any distance metric (not just Euclidean)

**Advantages:**
- ‚úì **Robust to outliers** (medoids not affected by extreme values)
- ‚úì Works with any distance metric
- ‚úì Interpretable centers (actual data points)

**Disadvantages:**
- ‚úó Still assumes spherical-ish clusters
- ‚úó Slower than K-Means (O(n¬≤) vs O(n))
- ‚úó Doesn't handle arbitrary shapes

**When to use:**
- Outliers present
- Need interpretable centers (real samples)
- Non-Euclidean distance (e.g., Manhattan, cosine)

**Python:**
```python
from sklearn_extra.cluster import KMedoids

clusterer = KMedoids(n_clusters=12, metric='euclidean')
labels = clusterer.fit_predict(X)
```

---

### **Affinity Propagation**

**Key Idea:** Clusters emerge from **message passing** between points about their suitability as exemplars.

**How it works:**
- Each point sends "responsibility" messages (how well-suited to be exemplar)
- Each point sends "availability" messages (how appropriate to choose as exemplar)
- Iterate until convergence
- Exemplars = cluster centers

**Advantages:**
- ‚úì **Automatic number of clusters**
- ‚úì No initialization
- ‚úì Works with any similarity measure

**Disadvantages:**
- ‚úó Very slow (O(n¬≤) iterations)
- ‚úó Memory intensive (similarity matrix)
- ‚úó Can be unstable (oscillations)

**When to use:**
- Don't know number of clusters
- Have good similarity measure
- Small-medium datasets

**Python:**
```python
from sklearn.cluster import AffinityPropagation

clusterer = AffinityPropagation(damping=0.9)
labels = clusterer.fit_predict(X)
```

---

## 6. Deep Learning-Based Methods

### **Deep Embedded Clustering (DEC)**

**Key Idea:** Learn representations and cluster assignments **jointly**.

**How it works:**
1. Pre-train autoencoder
2. Initialize cluster centers (e.g., K-Means on latent space)
3. Iteratively:
   - Compute soft assignments using Student's t-distribution
   - Update encoder to improve cluster purity

**Advantages:**
- ‚úì End-to-end learning
- ‚úì Nonlinear manifold learning
- ‚úì Can handle complex, non-Gaussian data

**Disadvantages:**
- ‚úó Requires neural network training
- ‚úó Hyperparameter tuning
- ‚úó Initialization sensitive

---

### **JULE (Joint Unsupervised Learning)**

**Key Idea:** Merge clustering and representation learning into single optimization.

**Why useful:**
- Standard VAE: Learn representation ‚Üí cluster (two-stage)
- JULE: Jointly optimize both

---

## 7. Hierarchical Methods

### **Agglomerative Clustering**

**Key Idea:** Bottom-up merging of clusters based on linkage criterion.

**Linkage types:**
- **Single**: Minimum distance between clusters ‚Üí can create elongated clusters
- **Complete**: Maximum distance ‚Üí compact spherical clusters
- **Average**: Mean distance ‚Üí balanced
- **Ward**: Minimize within-cluster variance ‚Üí similar to K-Means

**Advantages:**
- ‚úì **Single linkage handles arbitrary shapes** (chain-like clusters)
- ‚úì Dendrogram visualization
- ‚úì No need to specify k upfront (cut tree at any level)

**Disadvantages:**
- ‚úó O(n¬≤) or O(n¬≥) complexity
- ‚úó Single linkage sensitive to noise (chaining effect)

**When to use:**
- Want cluster hierarchy
- Elongated/arbitrary shapes (single linkage)
- Moderate dataset size

**Python:**
```python
from sklearn.cluster import AgglomerativeClustering

clusterer = AgglomerativeClustering(n_clusters=12, linkage='single')
labels = clusterer.fit_predict(X)
```

---

## Summary Table: Method Selection Guide

| Method | Best For | Cluster Shape | Outlier Robust | Auto k | Speed |
|--------|----------|---------------|----------------|--------|-------|
| **K-Means** | Spherical, balanced | Spherical | No | No | Very Fast |
| **GMM (full)** | Elliptical, probabilistic | Elliptical | No | BIC/AIC | Fast |
| **t-Mixture** | Heavy tails, outliers | Elliptical | **Yes** | BIC/AIC | Medium |
| **DBSCAN** | Arbitrary shapes, similar density | Arbitrary | **Yes** | **Yes** | Fast |
| **HDBSCAN** | Arbitrary shapes, varying density | Arbitrary | **Yes** | **Yes** | Medium |
| **Mean Shift** | Arbitrary shapes, small data | Arbitrary | Moderate | **Yes** | Slow |
| **Spectral** | Manifolds, non-convex | Manifolds | No | No | Slow |
| **K-Medoids** | Outliers, need actual centers | Spherical-ish | **Yes** | No | Medium |
| **Agglom (single)** | Chain-like, hierarchy | Arbitrary | No | Cut tree | Slow |
| **Agglom (ward)** | Spherical, hierarchy | Spherical | No | Cut tree | Slow |

---

## Recommendations for VAE Latent Space

Based on your VAE analysis:

### ‚úÖ **Use GMM (full covariance)**
**Why:**
- Latent clusters are **compact** (HDBSCAN failed ‚Üí no density gradients)
- Latent clusters are **elliptical** (GMM > K-Means by 13%)
- Latent space is **non-Gaussian but not arbitrary-shaped**
- Full covariance captures different variances per dimension

### ‚ùå **Avoid:**
- **HDBSCAN**: Latent space has uniform density (no gradients to exploit)
- **Mean Shift**: Too slow, no advantage over GMM for elliptical clusters
- **t-Mixture**: VAE already handles outliers via reconstruction loss
- **Spectral**: Latent space already low-dimensional and well-structured

### ü§î **Worth Testing:**

**1. Hierarchical Ward Clustering**
- May reveal cluster hierarchy (lithology sub-types)
- Dendrogram shows relationships

**2. Subspace Clustering**
- Some latent dimensions collapsed ‚Üí effective dimensionality < 8
- Different lithologies may use different subspaces

**3. Deep Embedded Clustering (DEC)**
- Joint optimization of VAE + clustering
- Could improve over two-stage approach

---

## Key Insight: Non-Gaussian ‚â† Arbitrary Shape

**Your latent space has:**
- ‚úó Non-Gaussian marginal distributions (Q-Q plots show deviations)
- ‚úó Correlated dimensions (violates N(0,I) prior)
- ‚úó Posterior collapse (some dims have std ‚â™ 1)

**BUT:**
- ‚úì Clusters are still **compact** (HDBSCAN failed)
- ‚úì Clusters are **elliptical** (GMM > K-Means)
- ‚úì Uniform density within clusters (no gradients)

**Conclusion:**
Non-Gaussian distributions can still produce elliptical, compact clusters. The VAE learned meaningful structure despite violating its own prior assumptions. This is actually **good** for clustering - the Œ≤ parameter optimization (Œ≤=0.5) **preserved feature correlations** which helps distinguish lithologies.

---

## Further Reading

- **GMM**: Bishop, "Pattern Recognition and Machine Learning" (2006), Chapter 9
- **HDBSCAN**: Campello et al., "Density-Based Clustering Based on Hierarchical Density Estimates" (2013)
- **t-Mixture**: Peel & McLachlan, "Robust mixture modelling using the t distribution" (2000)
- **Spectral**: Von Luxburg, "A tutorial on spectral clustering" (2007)
- **DEC**: Xie et al., "Unsupervised Deep Embedding for Clustering Analysis" (2016)
