%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AGUtmpl.tex: this template file is for articles formatted with LaTeX2e,
% Modified December 2018
%
% This template includes commands and instructions
% given in the order necessary to produce a final output that will
% satisfy AGU requirements.
%
% FOR FIGURES, DO NOT USE \psfrag
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% IMPORTANT NOTE:
%
% SUPPORTING INFORMATION DOCUMENTATION IS NOT COPYEDITED BEFORE PUBLICATION.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Step 1: Set the \documentclass
%
%
% PLEASE USE THE DRAFT OPTION TO SUBMIT YOUR PAPERS.
% The draft option produces double spaced output.
%
% Choose the journal abbreviation for the journal you are
% submitting to:

% jgrga JOURNAL OF GEOPHYSICAL RESEARCH (use for all of them)
% gbc   GLOBAL BIOCHEMICAL CYCLES
% grl   GEOPHYSICAL RESEARCH LETTERS
% pal   PALEOCEANOGRAPHY
% ras   RADIO SCIENCE
% rog   REVIEWS OF GEOPHYSICS
% tec   TECTONICS
% wrr   WATER RESOURCES RESEARCH
% gc    GEOCHEMISTRY, GEOPHYSICS, GEOSYSTEMS
% sw    SPACE WEATHER
% ms    JAMES
% ef    EARTH'S FUTURE
%
%
%
% (If you are submitting to a journal other than jgrga,
% substitute the initials of the journal for "jgrga" below.)

% \documentclass[draft,jgrga]{agutexSI2019}
\documentclass[draft,jgrga]{agutexSI2019}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  SUPPORTING INFORMATION TEMPLATE
%
%% ------------------------------------------------------------------------ %%
%
%
%Please use this template when formatting and submitting your Supporting Information.

%This template serves as both a “table of contents” for the supporting information for your article and as a summary of files.
%
%
%OVERVIEW
%
%Please note that all supporting information will be peer reviewed with your manuscript. It will not be copyedited if the paper is accepted.
%In general, the purpose of the supporting information is to enable authors to provide and archive auxiliary information such as data tables, method information, figures, video, or computer software, in digital formats so that other scientists can use it.
%The key criteria are that the data:
% 1. supplement the main scientific conclusions of the paper but are not essential to the conclusions (with the exception of
%    including %data so the experiment can be reproducible);
% 2. are likely to be usable or used by other scientists working in the field;
% 3. are described with sufficient precision that other scientists can understand them, and
% 4. are not exe files.
%
%USING THIS TEMPLATE
%
%***All references should be included in the reference list of the main paper so that they can be indexed, linked, and counted as citations.  The reference section does not count toward length limits.
%
%All Supporting text and figures should be included in this document. Insert supporting information content into each appropriate section of the template. To add additional captions, simply copy and paste each sample as needed.

%Tables may be included, but can also be uploaded separately, especially if they are larger than 1 page, or if necessary for retaining table formatting. Data sets, large tables, movie files, and audio files should be uploaded separately. Include their captions in this document and list the file name with the caption. You will be prompted to upload these files on the Upload Files tab during the submission process, using file type “Supporting Information (SI)”

%IMPORTANT NOTE ON FIGURES AND TABLES
% Placeholders for figures and tables appear after the \end{article} command, after references.
% DO NOT USE \psfrag or \subfigure commands.
%
\usepackage{xcolor}
\newcommand{\newtext}[1]{\textcolor{orange}{#1}}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
%
%  Uncomment the following command to allow illustrations to print
%   when using Draft:
 \setkeys{Gin}{draft=False}
%
% You may need to use one of these options for graphicx depending on the driver program you are using. 
%
% [xdvi], [dvipdf], [dvipsone], [dviwindo], [emtex], [dviwin],
% [pctexps],  [pctexwin],  [pctexhp],  [pctex32], [truetex], [tcidvi],
% [oztex], [textures]
%
%
%% ------------------------------------------------------------------------ %%
%
%  ENTER PREAMBLE
%
%% ------------------------------------------------------------------------ %%

% Author names in capital letters:
%\authorrunninghead{BALES ET AL.}

% Shorter version of title entered in capital letters:
%\titlerunninghead{SHORT TITLE}

%Corresponding author mailing address and e-mail address:
%\authoraddr{Corresponding author: A. B. Smith,
%Department of Hydrology and Water Resources, University of
%Arizona, Harshbarger Building 11, Tucson, AZ 85721, USA.
%(a.b.smith@hwr.arizona.edu)}

\begin{document}

%% ------------------------------------------------------------------------ %%
%
%  TITLE
%
%% ------------------------------------------------------------------------ %%

%\includegraphics{agu_pubart-white_reduced.eps}


\title{Supporting Information for ``An AI-enabled data processing pipeline for ingesting borehole data in peridotite environments''}
%
% e.g., \title{Supporting Information for "Terrestrial ring current:
% Origin, formation, and decay $\alpha\beta\Gamma\Delta$"}
%
%DOI: 10.1002/%insert paper number here%

%% ------------------------------------------------------------------------ %%
%
%  AUTHORS AND AFFILIATIONS
%
%% ------------------------------------------------------------------------ %%


% List authors by first name or initial followed by last name and
% separated by commas. Use \affil{} to number affiliations, and
% \thanks{} for author notes.
% Additional author notes should be indicated with \thanks{} (for
% example, for current addresses).

% Example: \authors{A. B. Author\affil{1}\thanks{Current address, Antartica}, B. C. Author\affil{2,3}, and D. E.
% Author\affil{3,4}\thanks{Also funded by Monsanto.}}

% \authors{=Authors=}


% \affiliation{1}{First Affiliation}
% \affiliation{2}{Second Affiliation}
% \affiliation{3}{Third Affiliation}
% \affiliation{4}{Fourth Affiliation}

% \affiliation{=number=}{=Affiliation Address=}
%(repeat as many times as is necessary)

\authors{ John M. Aiken\affil{1,2,3}, Elliot Dufornet\affil{1}, Hamed Amiri\affil{3}, Lotta Ternieten\affil{3, 4}, Oliver Plümper\affil{3}}

\affiliation{1}{Njord Centre, Departments of Physics and Geosciences, University of Oslo, PO BOX 1048, Blindern, Oslo, 0316, Oslo, Norway}

\affiliation{2}{Expert Analytics, Oslo, Norway}

\affiliation{3}{Department of Earth Sciences, Utrecht University, Utrecht, The Netherlands}

\affiliation{4}{Department of Ocean Systems, Royal Netherlands Institute for Sea Research, The Netherlands}

%% ------------------------------------------------------------------------ %%
%
%  BEGIN ARTICLE
%
%% ------------------------------------------------------------------------ %%

% The body of the article must start with a \begin{article} command
%
% \end{article} must follow the references section, before the figures
%  and tables.

\begin{article}

%% ------------------------------------------------------------------------ %%
%
%  TEXT
%
%% ------------------------------------------------------------------------ %%

\noindent\textbf{Contents of this file}
\begin{enumerate}
\item Text S1: Model Architecture Evolution
\item Text S2: Failed Experimental Approaches
\item Text S3: Transfer Learning Failure Analysis
\item Text S4: Hyperparameter Optimization Methodology
\item Text S5: Distribution-Aware Feature Scaling
\item Text S6: $\beta$ Annealing Schedule Analysis
\item Table S1: Complete Model Performance Comparison
\item Table S2: Ablation Study Results
\item Table S3: High-Purity Cluster Analysis
\item Table S4: Transfer Learning Performance Summary
\item Figure S1: Raw Feature Distributions (6 histograms)
\item Figure S2: Feature Correlation Analysis (matrix + key scatter plots)
\item Figure S3: Cross-Modal Feature Patterns Analysis
\item Figure S4: Validation Loss vs Clustering Performance (v2.6 vs v2.10 VampPrior)
\item Figure S5: Latent Space Distribution Analysis
\item Figure S6: Model Architecture and Performance
\end{enumerate}

\noindent\textbf{Additional Supporting Information (Files uploaded separately)}
\begin{enumerate}
\item Dataset S1: Preprocessed training data (vae\_training\_data\_v2\_20cm.csv)
\item Code S1: Model implementations and training scripts
\item Model Checkpoints S1-S10: Trained model weights for all variants
\end{enumerate}

%% ------------------------------------------------------------------------ %%
%  TEXT S1
%% ------------------------------------------------------------------------ %%

\section*{Text S1: Model Architecture Evolution}

This supplement documents the complete development history of variational autoencoder (VAE) models for unsupervised lithology clustering from IODP borehole physical property data. We tested 11 distinct model variants, exploring different input features, preprocessing strategies, architectural choices, loss functions, and training procedures. Table S1 provides a complete performance comparison.

\subsection*{S1.1 Baseline Models}

\textbf{VAE MAD (Legacy).} Initial proof-of-concept using discrete MAD measurements: porosity, grain density, P-wave velocity, and thermal conductivity. Training dataset: 151 co-located samples from 21 boreholes. Architecture: 4D input, [16, 8] hidden layers, 2D and 8D latent variants. Limited by sparse co-location of discrete measurements.

\textbf{VAE GRA v1.} Paradigm shift to continuous MSCL measurements with 20cm depth binning. Input features: GRA bulk density, magnetic susceptibility (MS), natural gamma radiation (NGR). Training dataset: 403,391 samples from 524 boreholes, 178 unique lithologies. Architecture: 3D input, [16, 8] hidden layers, 8D latent space. Performance: ARI = 0.084 (k=20), Silhouette = 0.413. The 20cm binning strategy enabled 2,671$\times$ more training samples than VAE MAD.

\textbf{VAE GRA v2.} Extended v1 by incorporating RGB color features from SHIL imaging. Input features: GRA, MS, NGR, R, G, B. Training dataset: 238,506 samples from 296 boreholes, 139 unique lithologies (41\% reduction due to RGB coverage). Architecture: 6D input, [32, 16] hidden layers, 8D latent space. Performance: ARI = 0.128 (k=20). Despite fewer samples, multimodal features improved clustering by 52\%, demonstrating visual appearance is as diagnostic as physical properties.

\subsection*{S1.2 Optimization Phase}

\textbf{VAE GRA v2.1.} Applied distribution-aware feature scaling (Text S4) to v2 architecture. Key innovation: feature-specific transforms matched to observed distributions (Gaussian for GRA, Poisson/bimodal for MS/NGR, log-normal for RGB) before standard scaling. Performance: ARI = 0.167 (k=12), 40\% improvement with identical model architecture.

\textbf{VAE GRA v2.5.} Optimized $\beta$ hyperparameter (KL divergence weight) using validation methodology (Text S3). Selected $\beta=0.5$ based on validation ARI. Performance: ARI = 0.241 (k=12), 44\% improvement over v2.1. Low $\beta$ preserves geologically meaningful feature correlations that aid clustering.

\textbf{VAE GRA v2.6 (Recommended).} Introduced $\beta$ annealing curriculum learning (Text S6). Schedule: $\beta$ linearly increases from 0.001 to 0.5 over 50 epochs. Performance: ARI = 0.19 $\pm$ 0.05 (entropy-balanced 5-fold CV, see Text S7), representing 7\% relative improvement over fixed $\beta=0.5$ and 54\% over v2.1. Training efficiency: 16 epochs (43\% faster than v2.5). High-purity clusters: 96.6\% gabbro, 80.4\% nannofossil ooze (single-split example).

%% ------------------------------------------------------------------------ %%
%  TEXT S2
%% ------------------------------------------------------------------------ %%

\section*{Text S2: Failed Experimental Approaches}

We document unsuccessful variants to guide future research.

\subsection*{S2.1 Spatial Context (v2.2)}

\textbf{Architecture.} 18D input: 6 features $\times$ 3 positions (above, current, below). Encoder: 18D$\rightarrow$[32, 16]$\rightarrow$8D latent. Decoder: 8D$\rightarrow$[16, 32]$\rightarrow$6D (reconstruct current only).

\textbf{Results.} ARI = 0.103, only 3.9\% improvement over v2.1 despite 3$\times$ input dimensionality. \textbf{Conclusion:} 20cm binning already captures local stratigraphic variations.

\subsection*{S2.2 Dual Encoders (v3)}

\textbf{Architecture.} Separate encoders for physical (GRA, MS, NGR) and visual (RGB) features, concatenation fusion to 8D latent. Parameters: 1,270 (60\% of v2.1).

\textbf{Results.} ARI = 0.091, 7.9\% degradation vs v2.1. \textbf{Conclusion:} Early fusion (unified encoder) outperforms late fusion. GRA-RGB correlations require joint encoding.

\subsection*{S2.3 VampPrior (v2.10)}

\textbf{Architecture.} Mixture of 50 posteriors as prior. Added 50 learnable pseudo-inputs (6D each). Parameters: 2,402.

\textbf{Results.} ARI = 0.261, only 1.2\% improvement over v2.6. \textbf{Critical issue:} Validation loss exploded 680\% (1.3$\rightarrow$10.2). Early stopping at epoch 10. Imputation failed. \textbf{Conclusion:} Marginal gain not worth severe overfitting.

\subsection*{S2.4 Masked Encoding (v2.11)}

\textbf{Motivation.} Train with random feature masking (inspired by BERT/MAE) to enable: (1) robust representations despite missing features, (2) missing data imputation (predict NGR+RGB from GRA+MS), (3) better generalization to incomplete datasets. Hypothesis: learning from partial inputs improves robustness without sacrificing clustering.

\textbf{Architecture.} Same as v2.6 (distribution-aware scaling, $\beta$ annealing). Modified training: randomly mask features during forward pass, reconstruct all features. Tested three masking strategies: (1) Block masking: 30\% samples missing all NGR+RGB, (2) Random masking: 15\% per feature independently, (3) Borehole-level masking: each borehole randomly masks 10\% of features for all its samples (most realistic).

\textbf{Results.} All masking strategies degraded clustering:
\begin{itemize}
\item Block NGR+RGB (30\%): ARI = 0.236 ($-8.5\%$ vs v2.6)
\item Random (15\%): ARI = 0.248 ($-3.8\%$ vs v2.6)
\item Borehole-level (10\%): ARI = 0.240 ($-6.9\%$ vs v2.6)
\end{itemize}

\textbf{Imputation quality.} Despite reasonable RMSE values, $R^2$ scores reveal complete failure:
\begin{itemize}
\item NGR imputation: $R^2 = -1.4$ to $-5.4$ (worse than predicting mean)
\item RGB imputation: $R^2 = -16.8$ to $-79.3$ (no correlation with true values)
\end{itemize}

Predictions cluster near feature means with random noise. Model cannot learn GRA+MS $\rightarrow$ NGR+RGB mappings due to weak feature correlations (from Fig. S2: GRA vs NGR $r=-0.13$, GRA vs RGB $r \approx -0.1$).

\textbf{Comparison to v2.6 reconstruction.} Standard v2.6 (all features as input) achieves excellent reconstruction quality:
\begin{itemize}
\item RGB: $R^2 = 0.95$ (RMSE = 13--15\% of mean)
\item GRA: $R^2 = 0.89$ (RMSE = 5.1\% of mean)
\item NGR: $R^2 = 0.85$ (RMSE = 27.5\% of mean)
\item MS: $R^2 = 0.61$ (RMSE = 180\% of mean, high due to extreme outliers)
\end{itemize}

\textbf{Conclusion.} Fundamental distinction between reconstruction (autoencoder) and imputation (conditional generation). VAEs excel at reconstructing complete inputs ($R^2 > 0.85$ for most features) but fail at predicting missing features from weakly correlated partial inputs ($R^2 < 0$). Masking during training degrades clustering ($-4\%$ to $-8\%$) without enabling useful imputation. Cannot optimize both objectives simultaneously. For clustering applications, v2.6 (no masking) superior. For imputation needs, dedicated conditional generative model with stronger feature relationships required.

%% ------------------------------------------------------------------------ %%
%  TEXT S3
%% ------------------------------------------------------------------------ %%

\section*{Text S3: Transfer Learning Failure Analysis}

All transfer learning approaches failed to improve upon v2.6, despite access to larger datasets or pre-training on additional boreholes. These results provide crucial insights into multi-modal learning for geoscience applications.

\subsection*{S3.1 Feature Quality Over Quantity (v2.6.1)}

\textbf{Motivation.} RGB camera coverage limits v2.6 to 296/534 boreholes (56\%). RSC reflectance spectroscopy available on 519 boreholes (97\%) enabled testing if L*a*b* color + more data could match RGB performance.

\textbf{Architecture.} 7D input: GRA, MS, NGR, RSC L*, RSC a*, RSC b*, MSP point magnetic susceptibility. Training dataset: 345,269 samples (+44.8\%) from 484 boreholes (+63.5\%). Distribution-aware scaling, $\beta$ annealing 0.001$\rightarrow$0.5 (identical to v2.6).

\textbf{Results.} ARI = 0.119 ($-$53.9\% vs v2.6), Silhouette = 0.211 ($-$46.3\% vs v2.6's 0.409). Despite 44\% more training data and 64\% more boreholes, performance degraded by 54\%.

\textbf{Conclusion.} \textbf{Feature quality $>$ dataset size}. RGB camera color captures diagnostic visual wavelengths for lithology identification. RSC L*a*b* designed for human perceptual uniformity, not geological features. Low silhouette scores (0.230 vs 0.428) confirm weak cluster structure, not just misalignment. RGB's limited coverage is worth maintaining for superior discrimination.

\subsection*{S3.2 Sequential Transfer Learning (v2.6.2)}

\textbf{Motivation.} Leverage 228 extra boreholes with physical properties (no RGB) via pre-training. Pre-train on GRA+MS+NGR (524 boreholes), then fine-tune with RGB (296 boreholes).

\textbf{Training Strategy.} Stage 1: Pre-train 3D$\rightarrow$8D VAE on v1 dataset (403K samples). Stage 2: Expand to 6D input, fine-tune with lower LR (5e$-$4) on v2.6 dataset (239K samples).

\textbf{Results.} ARI = 0.125 ($-$51.4\%), Silhouette = 0.230 ($-$46\%). Training time: 406s (3.5$\times$ slower than v2.6's 115s).

\textbf{Conclusion.} Pre-trained latent space optimized for physical-only patterns (GRA$\leftrightarrow$MS$\leftrightarrow$NGR). RGB forced to adapt to pre-existing 8D space. Lower LR preserves pre-training but prevents sufficient RGB integration. Cross-modal correlations (``dark + dense = basalt'') not discovered.

\subsection*{S3.3 RGB Without Physical Context (v2.6.3)}

\textbf{Motivation.} Test if RGB color alone is sufficient for lithology clustering, or if physical properties (GRA, MS, NGR) are necessary.

\textbf{Architecture.} 3D input (R, G, B only). Log scaling + StandardScaler. Same training procedure as v2.6 (239K samples, $\beta$ annealing).

\textbf{Results.} ARI = 0.054 ($-$79.1\% vs v2.6), Silhouette = 0.530 (+29\% vs v2.6). \textbf{High silhouette + low ARI = well-separated but WRONG clusters}. Worse than physical-only v1 (ARI = 0.084, $-$36\% better).

\textbf{Key Insight.} RGB alone is ambiguous: dark material could be clay, basalt, or organic-rich mud (RGB cannot distinguish). Light material could be limestone, sand, or weathered basalt. Visual appearance without physical context creates clusters based on color, not geological composition.

\textbf{Synergistic Effect.} RGB alone (0.054) + Physical alone (0.084) $\ll$ Combined (0.258). Multi-modal fusion is not additive---cross-modal correlations are primary. Example lithology signatures require both modalities:
\begin{itemize}
\item Basalt: Dark + Dense + Magnetic + Low NGR
\item Clay: Dark + Light + Non-magnetic + High NGR
\item Carbonate Ooze: Light + Light + Non-magnetic + Low NGR
\end{itemize}

See Figure S3 for cross-modal pattern visualization demonstrating each lithology occupies unique regions in 6D feature space.

\subsection*{S3.4 Dual Pre-training (v2.6.4)}

\textbf{Motivation.} v2.6.2 failed because RGB forced into physical latent space. Test if giving each modality its own latent space enables better fusion.

\textbf{Training Strategy.} Stage 1a: Pre-train physical encoder (3D$\rightarrow$4D latent) on v1 dataset (524 boreholes). Stage 1b: Pre-train RGB encoder (3D$\rightarrow$4D latent) on v2.6 RGB (296 boreholes). Stage 2: Concatenate to 8D (4D+4D), train dual decoders that see full 8D to learn fusion.

\textbf{Results.} ARI = 0.120 ($-$53.6\%), Silhouette = 0.297 ($-$27\%). Training time: 712s (6.2$\times$ slower: 271s + 167s + 274s).

\textbf{Conclusion.} Both encoders pre-optimized for wrong objectives (within-modality reconstruction, not cross-modal clustering). Fusion layer cannot overcome suboptimal encoder representations. Multi-modal learning is \textbf{not compositional}: optimal(A) + optimal(B) $\neq$ optimal(A+B).

\subsection*{S3.5 Fundamental Insights}

\textbf{Why Joint Training Wins:}
\begin{enumerate}
\item \textbf{Pre-training optimizes for wrong objective.} Reconstruction $\neq$ cross-modal clustering. Pre-trained encoders learn within-modality patterns, not between-modality correlations.
\item \textbf{Cross-modal correlations are primary, not secondary.} Lithology discrimination requires learning ``dark + dense = basalt'' jointly. Sequential or separate training cannot discover these patterns.
\item \textbf{Representational commitment.} Pre-trained weights create inductive biases that resist adapting to new objectives (cross-modal clustering).
\item \textbf{Feature interactions $>$ individual features.} Combining features linearly (fusion layers) insufficient when underlying encoders optimized independently.
\end{enumerate}

\textbf{Recommendation:} For multi-modal clustering tasks, joint training from scratch is optimal. The 228 extra physical-only boreholes do not help because they lack the RGB modality needed to learn cross-modal correlations. See Table S4 for performance summary.

%% ------------------------------------------------------------------------ %%
%  TEXT S4
%% ------------------------------------------------------------------------ %%

\section*{Text S4: Hyperparameter Optimization Methodology}

Proper hyperparameter selection requires validation set usage, reserving test set for final unbiased evaluation.

\subsection*{S4.1 Data Splitting}

All models use borehole-level splits (70\%/15\%/15\%) to prevent spatial autocorrelation leakage. Results: Train (207 boreholes, 166,954 samples), Validation (38 boreholes, 35,755 samples), Test (51 boreholes, 35,797 samples).

\subsection*{S4.2 $\beta$ Selection (v2.5)}

Correct workflow: (1) Train with $\beta \in \{0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0\}$ on training set; (2) Evaluate on validation set; (3) Select $\beta^* = \arg\max_\beta \text{ARI}_{\text{val}}$; (4) Evaluate on test set once. Selected: $\beta^* = 0.5$, unbiased test ARI = 0.241.

%% ------------------------------------------------------------------------ %%
%  TEXT S5
%% ------------------------------------------------------------------------ %%

\section*{Text S5: Distribution-Aware Feature Scaling}

Standard preprocessing assumes Gaussian distributions. IODP data violate this (Figure S1).

\subsection*{S5.1 Feature Distributions}

\textbf{GRA:} Gaussian ($\mu=1.62$, $\sigma=0.42$ g/cm$^3$). Transform: StandardScaler only.

\textbf{MS, NGR:} Right-skewed Poisson/bimodal. Transform: Signed log $\text{sign}(x) \cdot \log(|x| + 1)$ + StandardScaler.

\textbf{R, G, B:} Log-normal (0--255). Transform: $\log(x + 1)$ + StandardScaler.

\subsection*{S5.2 Impact}

v2 (StandardScaler): ARI = 0.128

v2.1 (Distribution-aware): ARI = 0.167

\textbf{+40\% improvement with identical architecture.}

%% ------------------------------------------------------------------------ %%
%  TEXT S6
%% ------------------------------------------------------------------------ %%

\section*{Text S6: $\beta$ Annealing Schedule Analysis}

$\beta$ annealing applies curriculum learning: start with low $\beta$ (reconstruction focus), gradually increase to target (compression).

\subsection*{S6.1 Implementation}

Linear schedule over 50 epochs:
\begin{equation}
\beta(t) = \begin{cases}
0.001 + (0.5 - 0.001) \times (t / 50) & \text{if } t < 50 \\
0.5 & \text{if } t \geq 50
\end{cases}
\end{equation}

\subsection*{S6.2 Results}

Fixed $\beta=0.5$: 28 epochs, ARI = 0.232

Anneal 0.001$\rightarrow$0.5: 16 epochs, ARI = 0.258

\textbf{43\% faster convergence, 11\% better clustering.} Training trajectory matters, not just final hyperparameter value.

%% ------------------------------------------------------------------------ %%
%  TEXT S7
%% ------------------------------------------------------------------------ %%

\section*{Text S7: Cross-Validation and Performance Variance}

Initial model development used single train-test splits (70\%/15\%/15\%) for computational efficiency. However, we discovered that reported performance was sensitive to split choice, necessitating rigorous cross-validation.

\subsection*{S7.1 The Lucky Split Problem}

Original v2.6 test set (45 boreholes, random\_state=42) achieved ARI = 0.286. Analysis revealed this test set had:

\begin{itemize}
\item Lower lithology diversity (entropy = 2.95 vs 3.12 full dataset)
\item Overrepresented distinctive lithologies (gabbro 7.5$\times$, silty claystone 5.4$\times$)
\item Missing 39\% of rare lithologies (85/139 present)
\end{itemize}

\textbf{Conclusion:} Original split was easier to cluster by chance, not representative of model's true performance.

\subsection*{S7.2 Cross-Validation Methodology}

We tested three stratification approaches:

\textbf{Random 5-fold CV:} Standard approach with random borehole splits. Result: Mean ARI = 0.186 $\pm$ 0.035 (95\% CI: [0.117, 0.256]).

\textbf{Geographic stratified CV:} Cluster boreholes into 5 regions by latitude/longitude, stratify folds to ensure regional representation. Result: Mean ARI = 0.196 $\pm$ 0.031.

\textbf{Entropy-balanced CV (adopted):} Stratify by each borehole's dominant lithology to ensure similar lithology diversity across folds. Result: Mean ARI = 0.19 $\pm$ 0.05 (95\% CI: [0.10, 0.28]).

All three approaches converged on ARI $\approx$ 0.19, confirming original result (0.286) was 33.5\% inflated.

\subsection*{S7.3 Entropy vs Performance Relationship}

Entropy-balanced CV results (Table S5):

\begin{itemize}
\item Fold 1 (entropy=2.955): ARI = 0.237
\item Fold 2 (entropy=2.927): ARI = 0.235
\item Fold 3 (entropy=2.974): ARI = 0.179
\item Fold 4 (entropy=3.116, highest): ARI = 0.132 (hardest, as expected)
\item Fold 5 (entropy=2.883, lowest): ARI = 0.168
\end{itemize}

Lower entropy correlates with slightly easier clustering, but variance remains high ($\pm$0.05). This reflects genuine geological heterogeneity across boreholes---some regions naturally have more distinctive lithological signatures than others.

\subsection*{S7.4 Implications}

\textbf{All reported ARI values in Table S1 are from single splits and likely optimistic by $\sim$30\%.} We only performed full CV for v2.6 due to computational cost (5$\times$ training). Relative comparisons between models remain valid (e.g., "v2.5 is 44\% better than v2.1") since all used the same test set. However, absolute performance should be interpreted as upper bounds.

\textbf{Recommended practice:} Use entropy-balanced stratified CV for final model evaluation. High variance ($\pm$0.05) is expected and reflects geological reality, not methodology flaw.

%% ------------------------------------------------------------------------ %%
%  TEXT S8
%% ------------------------------------------------------------------------ %%

\section*{Text S8: Extreme $\beta$ Annealing (v2.6.7)}

After discovering that original v2.6.6 performance was inflated by lucky split, we explored extreme $\beta$ annealing schedules to recover performance on balanced folds.

\subsection*{S8.1 Motivation}

Starting from pure autoencoder ($\beta \approx 0$) may learn better initial reconstructions before adding KL regularization. Testing $\beta_{\text{end}}$ values beyond 0.5 to find optimal balance between regularization and preserving feature correlations.

\subsection*{S8.2 Tested Schedules}

On original lucky split (random\_state=42):

\begin{itemize}
\item \textbf{v2.6.6 baseline}: $\beta$: 0.001 $\rightarrow$ 0.5, ARI = 0.241
\item $\beta$: 1e-10 $\rightarrow$ 0.5, ARI = 0.271 (+12.4\%)
\item \textbf{$\beta$: 1e-10 $\rightarrow$ 0.75, ARI = 0.292 (+21.3\%)} $\leftarrow$ Best
\item $\beta$: 1e-10 $\rightarrow$ 1.0, ARI = 0.229 ($-$5.0\%)
\end{itemize}

\subsection*{S8.3 Key Findings}

\begin{itemize}
\item \textbf{Starting at $\beta \approx 0$ helps}: Pure autoencoder initialization (1e-10) outperforms 0.001
\item \textbf{Sweet spot at $\beta_{\text{end}}=0.75$}: Optimal balance between regularization and preserving correlations
\item \textbf{$\beta_{\text{end}}=1.0$ too high}: Standard VAE forces excessive disentanglement, destroys clustering signal
\item \textbf{Longer annealing doesn't help}: 50 epochs optimal; 75-100 epochs degrades performance
\end{itemize}

\subsection*{S8.4 Entropy-Balanced CV Validation}

Tested best schedule ($\beta$: 1e-10 $\rightarrow$ 0.75) with entropy-balanced stratified CV (Table S6):

\begin{itemize}
\item \textbf{v2.6.7}: ARI = 0.196 $\pm$ 0.037 (95\% CI: [0.124, 0.268])
\item \textbf{v2.6.6}: ARI = 0.190 $\pm$ 0.045 (95\% CI: [0.102, 0.278])
\item \textbf{Improvement}: +2.9\% performance, $-$18\% variance
\end{itemize}

\subsection*{S8.5 Why $\beta=0.75$ Works}

Lower $\beta$ preserves geologically meaningful feature correlations:

\begin{itemize}
\item Dark color + high density + low radioactivity = \textbf{basalt}
\item Light color + low density + biogenic texture = \textbf{carbonate ooze}
\item High radioactivity + clay minerals = \textbf{terrigenous clay}
\end{itemize}

Standard VAE ($\beta=1.0$) forces latent dimensions to be independent, destroying these correlations. $\beta=0.75$ provides enough regularization to prevent posterior collapse while retaining clustering signal.

%% ------------------------------------------------------------------------ %%
%  TABLES
%% ------------------------------------------------------------------------ %%

\begin{table}[h]
\caption{Complete Model Performance Comparison}
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
Model & Features & Samples & Boreholes & ARI (k=12) & Status \\
\midrule
VAE MAD & 4D discrete & 151 & 21 & --- & Legacy \\
VAE GRA v1 & 3D MSCL & 403K & 524 & 0.084$^\dagger$ & Superseded \\
VAE GRA v2 & 6D MSCL+RGB & 239K & 296 & 0.128$^\dagger$ & Superseded \\
\midrule
VAE GRA v2.1 & 6D (dist-aware, $\beta$=1.0) & 239K & 296 & 0.167$^\dagger$ & Baseline \\
VAE GRA v2.5 & 6D ($\beta$=0.5) & 239K & 296 & 0.241$^\dagger$ & Good \\
v2.6.6 & 10D ($\beta$: 0.001$\rightarrow$0.5) & 239K & 296 & 0.19 $\pm$ 0.05$^*$ & Good \\
\textbf{v2.6.7} & \textbf{10D ($\beta$: 1e-10$\rightarrow$0.75)} & \textbf{239K} & \textbf{296} & \textbf{0.196 $\pm$ 0.037$^*$} & \textbf{Best} \\
\midrule
\multicolumn{6}{l}{\textit{Failed Experiments - Architectural:}} \\
v2.2 & 18D spatial & 239K & 296 & 0.103$^\dagger$ & $-$38\% \\
v3 & 6D dual-encoder & 239K & 296 & 0.091$^\dagger$ & $-$45\% \\
v2.10 & 6D + VampPrior & 239K & 296 & 0.261$^\dagger$ & +1\%, val loss +680\% \\
v2.11 & 6D + masking & 239K & 296 & 0.240$^\dagger$ & $-$7\% \\
\midrule
\multicolumn{6}{l}{\textit{Failed Experiments - Transfer Learning:}} \\
v2.6.1 & 7D RSC+MSP & 345K & 484 & 0.119$^\dagger$ & $-$54\%, +44\% data \\
v2.6.2 & 6D (phys$\rightarrow$RGB) & 239K & 296 & 0.125$^\dagger$ & $-$51\%, 3.5$\times$ slower \\
v2.6.3 & 3D RGB-only & 239K & 296 & 0.054$^\dagger$ & $-$79\%, worst \\
v2.6.4 & 6D dual pre-train & 239K & 296 & 0.122$^\dagger$ & $-$54\%, 6.2$\times$ slower \\
\bottomrule
\end{tabular}
\begin{flushleft}
\footnotesize
$^*$Entropy-balanced 5-fold CV (Text S7). $^\dagger$Single train-test split (likely optimistic by $\sim$30\%).
\end{flushleft}
\label{tab:models}
\end{table}

\begin{table}[h]
\caption{Ablation Study: Component Contributions}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
Component & Model & Relative Improvement \\
\midrule
Baseline (standard scaling, $\beta$=1.0) & v2 & --- \\
+ Distribution-aware scaling & v2.1 & +30.5\% \\
+ $\beta$ optimization ($\beta$=0.5) & v2.5 & +44.3\% \\
+ $\beta$ annealing & v2.6 & +7.1\% \\
\midrule
\textbf{Total} & \textbf{v2.6 vs v2} & \textbf{+101.6\%} \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\small
\textbf{Note:} Relative improvements from same test set (all models use identical split). Absolute ARI values likely optimistic by $\sim$30\% (see Table S5, Text S7). Relative comparisons remain valid.
\label{tab:ablation}
\end{table}

\begin{table}[h]
\caption{High-Purity Clusters (v2.6, k=12)}
\centering
\begin{tabular}{@{}clcc@{}}
\toprule
Cluster & Dominant Lithology & Purity & Size \\
\midrule
3 & Gabbro & 96.6\% & 1,847 \\
7 & Nannofossil ooze & 80.4\% & 8,234 \\
1 & Clay & 39.2\% & 5,612 \\
9 & Basalt & 62.1\% & 1,023 \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\small
\textbf{Note:} From single train-test split (random\_state=42). Cluster purity varies across CV folds but gabbro consistently achieves $>$85\% purity.
\label{tab:clusters}
\end{table}

\begin{table}[h]
\caption{Transfer Learning Performance Summary}
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
Model & Approach & Samples & Boreholes & ARI$^\dagger$ & vs v2.6 & Train Time \\
\midrule
\textbf{v2.6} & \textbf{Joint training} & \textbf{239K} & \textbf{296} & \textbf{0.258} & \textbf{---} & \textbf{115s} \\
\midrule
v2.6.1 & More data (RSC+MSP) & 345K & 484 & 0.119 & $-$54\% & 117s \\
v2.6.2 & Sequential (phys$\rightarrow$RGB) & 239K & 296 & 0.125 & $-$51\% & 406s \\
v2.6.3 & RGB-only & 239K & 296 & 0.054 & $-$79\% & 176s \\
v2.6.4 & Dual pre-training & 239K & 296 & 0.122 & $-$54\% & 712s \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\small
\textbf{Key Findings:}
\begin{itemize}
\item All transfer learning approaches fail ($-$51\% to $-$79\% relative degradation)
\item More data doesn't help: v2.6.1 with +44\% samples performs $-$54\% worse
\item RGB alone is ambiguous: v2.6.3 worst performance ($-$79\%)
\item Joint training from scratch is optimal for multi-modal clustering
\item The 228 extra physical-only boreholes provide no benefit
\end{itemize}

$^\dagger$Single train-test split (same across all models). Relative degradation remains valid. Absolute ARI likely optimistic by $\sim$30\% (see Text S7).
\label{tab:transfer}
\end{table}

\begin{table}[h]
\caption{Entropy-Balanced Cross-Validation Results (v2.6)}
\centering
\begin{tabular}{@{}cccccccc@{}}
\toprule
Fold & Train & Test & Train & Test & Test Entropy & Best k & ARI \\
& Samples & Samples & Entropy & Entropy & Diff & & \\
\midrule
1 & 161,659 & 45,716 & 3.099 & 2.955 & 0.164 & 10 & 0.237 \\
2 & 155,507 & 49,461 & 3.113 & 2.927 & 0.192 & 12 & 0.235 \\
3 & 163,286 & 49,143 & 3.120 & 2.974 & 0.144 & 12 & 0.179 \\
4 & 166,652 & 42,815 & 3.084 & 3.116 & 0.002 & 15 & 0.132 \\
5 & 157,033 & 51,371 & 3.113 & 2.883 & 0.235 & 10 & 0.168 \\
\midrule
\multicolumn{7}{l}{\textbf{Mean $\pm$ Std}} & \textbf{0.190 $\pm$ 0.045} \\
\multicolumn{7}{l}{\textbf{95\% CI}} & \textbf{[0.102, 0.278]} \\
\multicolumn{7}{l}{\textbf{Original single split (random\_state=42)}} & \textbf{0.286} \\
\multicolumn{7}{l}{\textbf{Performance loss from biased split}} & \textbf{33.5\%} \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\small
\textbf{Notes:} Full dataset entropy = 3.119. Stratified 5-fold CV by dominant lithology. Each fold tests GMM clustering with k $\in$ \{10, 12, 15, 18\}, reports best. Fold 4 has highest test entropy (3.116, most diverse) and lowest ARI (0.132), confirming difficulty correlates with diversity. Original result outside 95\% CI, indicating lucky split.
\label{tab:cv_v2_6_6}
\end{table}

\begin{table}[h]
\caption{Entropy-Balanced Cross-Validation Results (v2.6.7)}
\centering
\begin{tabular}{@{}cccccccc@{}}
\toprule
Fold & Train & Test & Train & Test & Test Entropy & Best k & ARI \\
& Samples & Samples & Entropy & Entropy & Diff & & \\
\midrule
1 & 161,659 & 45,716 & 3.099 & 2.955 & 0.164 & 10 & 0.243 \\
2 & 155,507 & 49,461 & 3.113 & 2.927 & 0.192 & 12 & 0.220 \\
3 & 163,286 & 49,143 & 3.120 & 2.974 & 0.144 & 12 & 0.176 \\
4 & 166,652 & 42,815 & 3.084 & 3.116 & 0.002 & 15 & 0.150 \\
5 & 157,033 & 51,371 & 3.113 & 2.883 & 0.235 & 10 & 0.189 \\
\midrule
\multicolumn{7}{l}{\textbf{Mean $\pm$ Std}} & \textbf{0.196 $\pm$ 0.037} \\
\multicolumn{7}{l}{\textbf{95\% CI}} & \textbf{[0.124, 0.268]} \\
\multicolumn{7}{l}{\textbf{v2.6.6 baseline}} & \textbf{0.190 $\pm$ 0.045} \\
\multicolumn{7}{l}{\textbf{Improvement}} & \textbf{+2.9\%} \\
\multicolumn{7}{l}{\textbf{Variance reduction}} & \textbf{$-$18\%} \\
\bottomrule
\end{tabular}
\vspace{0.5em}

\small
\textbf{Notes:} $\beta$ schedule: 1e-10 $\rightarrow$ 0.75 over 50 epochs. Starting from pure autoencoder ($\beta \approx 0$) and annealing to $\beta=0.75$ (vs 0.5 in v2.6.6) achieves +2.9\% improvement and 18\% variance reduction. Sweet spot at $\beta_{\text{end}}=0.75$ balances regularization without destroying geologically meaningful feature correlations needed for clustering.
\label{tab:cv_v2_6_7}
\end{table}

\clearpage

%% ------------------------------------------------------------------------ %%
%  FIGURES
%% ------------------------------------------------------------------------ %%

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figS1_feature_distributions.png}
\caption{\textbf{Figure S1. Raw Feature Distributions.} Histograms for 6 input features (238,506 samples). (a) GRA: Gaussian distribution ($\mu=1.62$, $\sigma=0.42$ g/cm$^3$). (b) MS: Right-skewed Poisson-like distribution ($-150$ to $+2000$ instr. units). (c) NGR: Bimodal distribution (0--60 cps). (d-f) R, G, B: Log-normal distributions (0--255). Distribution-aware scaling applies feature-specific transforms (signed log for MS/NGR, log for RGB) before StandardScaler to normalize non-Gaussian features.}
\label{fig:distributions}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figS2_feature_correlations.png}
\caption{\textbf{Figure S2. Feature Correlation Analysis.} (a) Correlation matrix heatmap showing relationships between all 6 input features. RGB channels strongly correlated (r $>$ 0.9), indicating shared visual information. Physical properties (GRA, MS, NGR) show weaker correlations with each other and with visual features. (b-f) Key scatter plots: (b) GRA vs MS (r=0.08), (c) GRA vs R (r=$-0.13$), (d) R vs G (r=0.92), (e) R vs B (r=0.93), (f) G vs B (r=0.99). High RGB inter-correlation justifies treating color as coherent visual signal. Low GRA-RGB correlation shows physical and visual features provide complementary information.}
\label{fig:correlations}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{cross_modal_feature_patterns.png}
\caption{\textbf{Figure S3. Cross-Modal Feature Patterns: Why Joint Training Works.} Each lithology occupies a unique region in 6D feature space defined by cross-modal correlations. For six representative lithologies (clay, basalt, nannofossil ooze, gabbro, sand, silty clay), we show: (1) color swatch displaying median RGB, (2) RGB distributions overlaid on global background, (3--5) physical property distributions (GRA, MS, NGR) with arrows indicating lithology median position and classification (LOW/MED/HIGH based on percentile). Key insights: (a) Visual similarity is ambiguous---dark clay and dark basalt have similar RGB but very different physical properties (clay: low density + high NGR, basalt: high density + high MS + low NGR). (b) Physical properties alone are insufficient---light-colored materials span wide density ranges. (c) Cross-modal correlations are diagnostic---each lithology defined by unique combination across ALL features, not individual features. This demonstrates why joint training (v2.6) vastly outperforms RGB-only (v2.6.3, $-$79\% relative degradation) and all transfer learning approaches that fail to learn these cross-modal patterns. Multi-modal fusion is synergistic, not additive.}
\label{fig:crossmodal}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{vae_v2_6_vs_v2_10_analysis.png}
\caption{\textbf{Figure S4. Validation Loss vs Clustering Performance Discrepancy.} Comparison of v2.6 (standard $\mathcal{N}(0,I)$ prior) and v2.10 (VampPrior with K=50 components). Top panels show training and validation loss trajectories. VampPrior validation loss explodes 680\% (1.3$\rightarrow$10.2) while training loss remains low, indicating severe overfitting. Bottom panels show $\beta$ annealing schedule (identical for both models) and clustering ARI comparison across k values. Despite catastrophic validation loss, VampPrior achieves marginally higher single-split ARI (0.261 vs 0.258, +1.2\%). This demonstrates that reconstruction loss is an imperfect proxy for clustering quality, but massive validation loss increase indicates fundamental generalization failure. Marginal gain not worth overfitting risk.}
\label{fig:vamp_analysis}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{latent_distribution_analysis.png}
\caption{\textbf{Figure S5. Latent Space Distribution Analysis.} v2.6, 8D latent space. Left panel shows marginal distributions for each latent dimension. Shapiro-Wilk normality tests: 0/8 dimensions are Gaussian (all p $<$ 0.001). Right panel shows correlation matrix. Maximum correlation: 0.962 (dimensions strongly dependent). Bottom panel shows standard deviations: 4 dimensions exhibit posterior collapse (std $\approx$ 0.01--0.03, should be $\sim$1 under $\mathcal{N}(0,I)$ prior). Model systematically violates Gaussian prior assumption but still achieves good clustering. Gaussian prior acts as regularization, not hard constraint. Latent structure is driven by data, not prior.}
\label{fig:latent}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figS6_architecture_performance.png}
\caption{\textbf{Figure S6. Model Architecture and Performance.} (a) VAE v2.6 architecture diagram. Encoder: 6D input $\rightarrow$ Linear(6$\rightarrow$32) $\rightarrow$ ReLU $\rightarrow$ BatchNorm $\rightarrow$ Dropout(0.1) $\rightarrow$ Linear(32$\rightarrow$16) $\rightarrow$ ReLU $\rightarrow$ BatchNorm $\rightarrow$ Dropout(0.1) $\rightarrow$ fc\_mu(16$\rightarrow$8), fc\_logvar(16$\rightarrow$8). Reparameterization: $z = \mu + \sigma \cdot \epsilon$ where $\epsilon \sim \mathcal{N}(0,I)$. Decoder: symmetric [16, 32] layers $\rightarrow$ 6D output. Total parameters: 2,102. (b) Clustering performance (ARI) vs number of clusters k. Results from single train-test split. Performance stable across k=10--15 range (mean ARI $\approx$ 0.19 across folds, see Table S5). High-purity clusters achievable: 96.6\% gabbro, 80.4\% nannofossil ooze at k=12 (example from one fold).}
\label{fig:arch_perf}
\end{figure}

\end{article}


\end{document}
