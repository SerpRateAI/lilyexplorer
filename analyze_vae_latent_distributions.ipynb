{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Latent Space Distribution Analysis\n",
    "\n",
    "**Objective:** Analyze the distributional properties of the 8D latent space from VAE GRA v2.6\n",
    "\n",
    "**Key Questions:**\n",
    "1. Are the latent dimensions Gaussian? (Q-Q plots)\n",
    "2. What is the density structure? (2D projections, KDE)\n",
    "3. Do we have spherical vs elliptical vs arbitrary-shaped clusters?\n",
    "4. Why did HDBSCAN fail if the data is non-Gaussian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, kstest, anderson, normaltest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/utig5/johna/bhai/ml_models')\n",
    "from vae_lithology_gra_v2_5_model import VAE, DistributionAwareScaler\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load VAE Model and Extract Latent Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('/home/utig5/johna/bhai/vae_training_data_v2_20cm.csv')\n",
    "\n",
    "feature_cols = ['Bulk density (GRA)', 'Magnetic susceptibility (instr. units)',\n",
    "                'NGR total counts (cps)', 'R', 'G', 'B']\n",
    "\n",
    "# Split by borehole\n",
    "unique_boreholes = df['Borehole_ID'].unique()\n",
    "train_boreholes, test_boreholes = train_test_split(\n",
    "    unique_boreholes, train_size=0.85, random_state=42\n",
    ")\n",
    "train_boreholes, val_boreholes = train_test_split(\n",
    "    train_boreholes, train_size=0.7/0.85, random_state=42\n",
    ")\n",
    "\n",
    "test_mask = df['Borehole_ID'].isin(test_boreholes)\n",
    "train_mask = df['Borehole_ID'].isin(train_boreholes)\n",
    "\n",
    "df_test = df[test_mask].copy()\n",
    "df_train = df[train_mask].copy()\n",
    "\n",
    "print(f\"Train: {len(df_train):,} samples\")\n",
    "print(f\"Test:  {len(df_test):,} samples\")\n",
    "\n",
    "# Prepare features\n",
    "X_train = df_train[feature_cols].values\n",
    "X_test = df_test[feature_cols].values\n",
    "y_test = df_test['Principal'].values\n",
    "\n",
    "# Scale\n",
    "scaler = DistributionAwareScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained VAE model\n",
    "print(\"\\nLoading VAE model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = VAE(input_dim=6, latent_dim=8, hidden_dims=[32, 16])\n",
    "checkpoint = torch.load('/home/utig5/johna/bhai/ml_models/checkpoints/vae_gra_v2_6_hdbscan_comparison.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.latent_dim}D latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent representations\n",
    "print(\"\\nExtracting latent representations...\")\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    mu, logvar = model.encode(X_tensor)\n",
    "    latent = mu.cpu().numpy()\n",
    "\n",
    "print(f\"Latent shape: {latent.shape}\")\n",
    "print(f\"Latent range: [{latent.min():.2f}, {latent.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Q Plots for All 8 Latent Dimensions\n",
    "\n",
    "Q-Q (quantile-quantile) plots compare the distribution of each latent dimension to a standard normal distribution.\n",
    "\n",
    "**What to look for:**\n",
    "- Points on diagonal line = Gaussian\n",
    "- S-curve = Heavy/light tails\n",
    "- Deviations = Non-Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(latent[:, i], dist=\"norm\", plot=ax)\n",
    "    \n",
    "    # Shapiro-Wilk test (sample for speed if needed)\n",
    "    sample_size = min(5000, len(latent))\n",
    "    sample_idx = np.random.choice(len(latent), sample_size, replace=False)\n",
    "    stat, p_value = shapiro(latent[sample_idx, i])\n",
    "    \n",
    "    # Mean and std\n",
    "    mean_val = latent[:, i].mean()\n",
    "    std_val = latent[:, i].std()\n",
    "    \n",
    "    ax.set_title(f'Dimension {i+1}\\nμ={mean_val:.3f}, σ={std_val:.3f}\\nShapiro p={p_value:.2e}', \n",
    "                 fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color code based on normality\n",
    "    if p_value > 0.05:\n",
    "        ax.get_lines()[0].set_color('green')\n",
    "        ax.get_lines()[0].set_alpha(0.6)\n",
    "    else:\n",
    "        ax.get_lines()[0].set_color('red')\n",
    "        ax.get_lines()[0].set_alpha(0.6)\n",
    "\n",
    "plt.suptitle('Q-Q Plots: Latent Dimensions vs Normal Distribution\\n(Green=Gaussian p>0.05, Red=Non-Gaussian p<0.05)', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_latent_qq_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQ-Q plots saved to: vae_latent_qq_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Formal Normality Tests\n",
    "\n",
    "Multiple statistical tests for normality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_results = []\n",
    "\n",
    "for i in range(8):\n",
    "    dim_data = latent[:, i]\n",
    "    \n",
    "    # Sample for tests (some have sample size limits)\n",
    "    sample_size = min(5000, len(dim_data))\n",
    "    sample_idx = np.random.choice(len(dim_data), sample_size, replace=False)\n",
    "    sample_data = dim_data[sample_idx]\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    shapiro_stat, shapiro_p = shapiro(sample_data)\n",
    "    \n",
    "    # Kolmogorov-Smirnov test (compare to normal with same mean/std)\n",
    "    ks_stat, ks_p = kstest(sample_data, 'norm', args=(dim_data.mean(), dim_data.std()))\n",
    "    \n",
    "    # Anderson-Darling test\n",
    "    anderson_result = anderson(sample_data, dist='norm')\n",
    "    anderson_stat = anderson_result.statistic\n",
    "    # Check at 5% significance level (index 2)\n",
    "    anderson_reject = anderson_stat > anderson_result.critical_values[2]\n",
    "    \n",
    "    # D'Agostino-Pearson test\n",
    "    dagostino_stat, dagostino_p = normaltest(sample_data)\n",
    "    \n",
    "    normality_results.append({\n",
    "        'Dimension': i+1,\n",
    "        'Mean': dim_data.mean(),\n",
    "        'Std': dim_data.std(),\n",
    "        'Skewness': stats.skew(dim_data),\n",
    "        'Kurtosis': stats.kurtosis(dim_data),\n",
    "        'Shapiro_p': shapiro_p,\n",
    "        'KS_p': ks_p,\n",
    "        'Anderson_stat': anderson_stat,\n",
    "        'Anderson_reject': anderson_reject,\n",
    "        'DAgostino_p': dagostino_p,\n",
    "        'Is_Gaussian_05': shapiro_p > 0.05 and ks_p > 0.05 and not anderson_reject\n",
    "    })\n",
    "\n",
    "df_normality = pd.DataFrame(normality_results)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"NORMALITY TEST RESULTS (α=0.05)\")\n",
    "print(\"=\"*100)\n",
    "print(df_normality.to_string(index=False))\n",
    "print()\n",
    "print(f\"Gaussian dimensions (all tests pass): {df_normality['Is_Gaussian_05'].sum()}/8\")\n",
    "print(f\"Non-Gaussian dimensions: {(~df_normality['Is_Gaussian_05']).sum()}/8\")\n",
    "print()\n",
    "\n",
    "# Save\n",
    "df_normality.to_csv('vae_latent_normality_tests.csv', index=False)\n",
    "print(\"Results saved to: vae_latent_normality_tests.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Histograms with Normal Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i]\n",
    "    dim_data = latent[:, i]\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = ax.hist(dim_data, bins=50, density=True, alpha=0.6, \n",
    "                                 color='blue', edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Fit normal distribution\n",
    "    mu, sigma = dim_data.mean(), dim_data.std()\n",
    "    x = np.linspace(dim_data.min(), dim_data.max(), 100)\n",
    "    normal_pdf = stats.norm.pdf(x, mu, sigma)\n",
    "    ax.plot(x, normal_pdf, 'r-', linewidth=2, label=f'Normal(μ={mu:.2f}, σ={sigma:.2f})')\n",
    "    \n",
    "    # Kernel Density Estimate\n",
    "    kde = stats.gaussian_kde(dim_data)\n",
    "    kde_pdf = kde(x)\n",
    "    ax.plot(x, kde_pdf, 'g--', linewidth=2, label='KDE (actual)')\n",
    "    \n",
    "    ax.set_xlabel(f'Latent Dimension {i+1}')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Dim {i+1}: Skew={stats.skew(dim_data):.2f}, Kurt={stats.kurtosis(dim_data):.2f}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Latent Dimension Distributions\\n(Red=Fitted Normal, Green=Actual KDE, Blue=Histogram)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_latent_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution plots saved to: vae_latent_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 2D Projections: Density Visualization\n",
    "\n",
    "Visualize the latent space structure in 2D to understand cluster shapes and density patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to 2D for visualization\n",
    "print(\"Computing PCA projection...\")\n",
    "pca = PCA(n_components=2)\n",
    "latent_2d = pca.fit_transform(latent)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_[0]:.1%} + {pca.explained_variance_ratio_[1]:.1%} = {pca.explained_variance_ratio_.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D projection with density contours\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Scatter with density contours\n",
    "ax = axes[0]\n",
    "\n",
    "# Hexbin for density\n",
    "hb = ax.hexbin(latent_2d[:, 0], latent_2d[:, 1], gridsize=50, cmap='viridis', \n",
    "               mincnt=1, alpha=0.8)\n",
    "plt.colorbar(hb, ax=ax, label='Point Density')\n",
    "\n",
    "# KDE contours\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(latent_2d[:, 0].min(), latent_2d[:, 0].max(), 100),\n",
    "    np.linspace(latent_2d[:, 1].min(), latent_2d[:, 1].max(), 100)\n",
    ")\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Fit KDE\n",
    "kde = stats.gaussian_kde(latent_2d.T)\n",
    "density = kde(positions.T).reshape(xx.shape)\n",
    "\n",
    "# Plot contours\n",
    "ax.contour(xx, yy, density, levels=10, colors='white', alpha=0.4, linewidths=1)\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=12)\n",
    "ax.set_ylabel('PC2', fontsize=12)\n",
    "ax.set_title('Latent Space Density (PCA projection)\\nHexbin + KDE Contours', fontsize=13)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Top lithologies colored\n",
    "ax = axes[1]\n",
    "\n",
    "# Get top N lithologies\n",
    "top_n = 10\n",
    "top_lithologies = pd.Series(y_test).value_counts().head(top_n).index.tolist()\n",
    "\n",
    "# Create color map\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, top_n))\n",
    "color_map = {lith: colors[i] for i, lith in enumerate(top_lithologies)}\n",
    "\n",
    "# Plot background (other lithologies)\n",
    "mask_other = ~pd.Series(y_test).isin(top_lithologies)\n",
    "ax.scatter(latent_2d[mask_other, 0], latent_2d[mask_other, 1], \n",
    "           c='lightgray', alpha=0.3, s=5, label='Other')\n",
    "\n",
    "# Plot top lithologies\n",
    "for lith in top_lithologies:\n",
    "    mask = y_test == lith\n",
    "    ax.scatter(latent_2d[mask, 0], latent_2d[mask, 1], \n",
    "               c=[color_map[lith]], alpha=0.6, s=10, label=lith)\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=12)\n",
    "ax.set_ylabel('PC2', fontsize=12)\n",
    "ax.set_title(f'Latent Space by Lithology\\nTop {top_n} lithologies colored', fontsize=13)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_latent_2d_density.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"2D density plots saved to: vae_latent_2d_density.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis\n",
    "\n",
    "Check if latent dimensions are independent (as VAE prior assumes) or correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = np.corrcoef(latent.T)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "plt.colorbar(im, ax=ax, label='Correlation')\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks(range(8))\n",
    "ax.set_yticks(range(8))\n",
    "ax.set_xticklabels([f'Dim {i+1}' for i in range(8)])\n",
    "ax.set_yticklabels([f'Dim {i+1}' for i in range(8)])\n",
    "\n",
    "# Annotate values\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        text = ax.text(j, i, f'{corr_matrix[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "ax.set_title('Latent Dimension Correlations\\n(VAE assumes independence, but...)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_latent_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print max correlation\n",
    "np.fill_diagonal(corr_matrix, 0)  # Ignore self-correlation\n",
    "max_corr = np.abs(corr_matrix).max()\n",
    "max_idx = np.unravel_index(np.abs(corr_matrix).argmax(), corr_matrix.shape)\n",
    "print(f\"\\nMax correlation: {corr_matrix[max_idx]:.3f} between Dim {max_idx[0]+1} and Dim {max_idx[1]+1}\")\n",
    "print(f\"Mean absolute correlation: {np.abs(corr_matrix).mean():.3f}\")\n",
    "print(\"\\nCorrelation matrix saved to: vae_latent_correlations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"LATENT SPACE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nShape: {latent.shape}\")\n",
    "print(f\"Device used for encoding: {device}\")\n",
    "print()\n",
    "\n",
    "summary_stats = []\n",
    "for i in range(8):\n",
    "    dim_data = latent[:, i]\n",
    "    summary_stats.append({\n",
    "        'Dimension': i+1,\n",
    "        'Mean': dim_data.mean(),\n",
    "        'Std': dim_data.std(),\n",
    "        'Min': dim_data.min(),\n",
    "        'Max': dim_data.max(),\n",
    "        'Range': dim_data.max() - dim_data.min(),\n",
    "        'Skewness': stats.skew(dim_data),\n",
    "        'Kurtosis': stats.kurtosis(dim_data)\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "print(df_summary.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Check for posterior collapse\n",
    "collapsed_dims = df_summary[df_summary['Std'] < 0.1]['Dimension'].tolist()\n",
    "if collapsed_dims:\n",
    "    print(f\"⚠️  POSTERIOR COLLAPSE detected in dimensions: {collapsed_dims}\")\n",
    "    print(f\"   (std < 0.1, should be ~1 for N(0,I) prior)\")\n",
    "else:\n",
    "    print(\"✓ No posterior collapse (all dimensions have std >= 0.1)\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*100)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"1. Gaussian dimensions: {df_normality['Is_Gaussian_05'].sum()}/8\")\n",
    "print(f\"2. Max dimension correlation: {max_corr:.3f}\")\n",
    "print(f\"3. Posterior collapse: {len(collapsed_dims)} dimensions\")\n",
    "print(f\"4. PCA variance explained (PC1+PC2): {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print()\n",
    "print(\"INTERPRETATION:\")\n",
    "if df_normality['Is_Gaussian_05'].sum() == 0:\n",
    "    print(\"  - ALL dimensions are NON-GAUSSIAN\")\n",
    "    print(\"  - VAE N(0,I) prior is violated\")\n",
    "    print(\"  - But clustering still works if structure is compact\")\n",
    "if max_corr > 0.5:\n",
    "    print(f\"  - Strong correlations (max {max_corr:.2f}) indicate dimensions are NOT independent\")\n",
    "    print(\"  - Disentanglement failed, but this may help clustering (preserves feature correlations)\")\n",
    "if len(collapsed_dims) > 0:\n",
    "    print(f\"  - {len(collapsed_dims)} dimensions collapsed → effective dimensionality < 8\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cluster Shape Analysis\n",
    "\n",
    "Analyze the shape of individual lithology clusters in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top lithologies\n",
    "top_lithologies = pd.Series(y_test).value_counts().head(5).index.tolist()\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"CLUSTER SHAPE ANALYSIS (Top 5 Lithologies)\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "\n",
    "for lith in top_lithologies:\n",
    "    mask = y_test == lith\n",
    "    lith_latent = latent[mask]\n",
    "    \n",
    "    print(f\"\\n{lith} (n={mask.sum()}):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Compute covariance matrix\n",
    "    cov = np.cov(lith_latent.T)\n",
    "    \n",
    "    # Eigenvalues indicate cluster shape\n",
    "    eigenvalues = np.linalg.eigvalsh(cov)\n",
    "    eigenvalues = eigenvalues[::-1]  # Sort descending\n",
    "    \n",
    "    # Sphericity: ratio of largest to smallest eigenvalue\n",
    "    sphericity = eigenvalues[-1] / eigenvalues[0] if eigenvalues[0] > 0 else 0\n",
    "    \n",
    "    print(f\"  Eigenvalues: {eigenvalues}\")\n",
    "    print(f\"  Sphericity ratio (small/large): {sphericity:.3f}\")\n",
    "    if sphericity > 0.5:\n",
    "        print(f\"  Shape: SPHERICAL (eigenvalues similar)\")\n",
    "    elif sphericity > 0.1:\n",
    "        print(f\"  Shape: ELLIPTICAL (moderate elongation)\")\n",
    "    else:\n",
    "        print(f\"  Shape: HIGHLY ELONGATED (cigar-shaped)\")\n",
    "    \n",
    "    # Volume (determinant)\n",
    "    det_cov = np.linalg.det(cov)\n",
    "    print(f\"  Volume (det(Σ)): {det_cov:.2e}\")\n",
    "    \n",
    "    # Trace (total variance)\n",
    "    trace_cov = np.trace(cov)\n",
    "    print(f\"  Total variance (tr(Σ)): {trace_cov:.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
